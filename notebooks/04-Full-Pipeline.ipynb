{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Pipeline\n",
    "In this section of the demo, we consolidate all previous steps for a full, end-to-end pipeline for incremental ingest, feature engineering, training, prediction, and evaluation.\n",
    "\n",
    "This will be integrated into **our company's orchestration framework** but showing it all in one place will allow our dev ops team to implement it. \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The ML engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the ML engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Set of python functions from the Data Engineer, Data Scientist, and ML Engineer.  \n",
    "Output: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile\t       dags\t\t\t    packages.txt\r\n",
      "README.md\t       dependencies\t\t    plugins\r\n",
      "airflow_settings.yaml  docker-compose.override.yml  requirements.txt\r\n",
      "citibike_ml\t       include\t\t\t    weather.csv\r\n",
      "conda-env.yml\t       k8s_yaml_files\t\t    xray\r\n",
      "creds.json\t       notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!ls /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import getpass\n",
    "import uuid\n",
    "\n",
    "with open('/code/include/creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    connection_parameters = {\n",
    "      'account': data['account'],\n",
    "      'user': data['username'],\n",
    "      'password': data['password'], #getpass.getpass(),\n",
    "      'role': data['role'],\n",
    "      'warehouse': data['warehouse']}\n",
    "\n",
    "session = snp.Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_db_name = 'CITIBIKEML_JF'\n",
    "project_schema_name = 'DEMO'\n",
    "project_db_schema = str(project_db_name)+'.'+str(project_schema_name)\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "model_id = str(uuid.uuid1()).replace('-', '_')\n",
    "\n",
    "download_base_url = 'https://s3.amazonaws.com/tripdata/'\n",
    "\n",
    "load_table_name = str(project_db_schema)+'.'+'RAW_'\n",
    "trips_table_name = str(project_db_schema)+'.'+'TRIPS'\n",
    "holiday_table_name = str(project_db_schema)+'.'+'HOLIDAYS'\n",
    "precip_table_name = str(project_db_schema)+'.'+'WEATHER'\n",
    "model_stage_name = str(project_db_schema)+'.'+'model_stage'\n",
    "clone_table_name = str(project_db_schema)+'.'+'CLONE_'+str(model_id)\n",
    "feature_view_name = str(project_db_schema)+'.'+'STATION_<station_id>_VIEW_'+str(model_id)\n",
    "pred_table_name = str(project_db_schema)+'.'+'PREDICTIONS_'+str(model_id)\n",
    "eval_table_name = str(project_db_schema)+'.'+'EVAL_'+str(model_id)\n",
    "load_stage_name = 'load_stage'\n",
    "\n",
    "_ = session.sql('USE DATABASE ' + str(project_db_name)).collect()\n",
    "_ = session.sql('USE SCHEMA ' + str(project_schema_name)).collect()\n",
    "\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS ' + str(model_stage_name)).collect()\n",
    "_ = session.sql('CREATE OR REPLACE TEMPORARY STAGE ' + str(load_stage_name)).collect()\n",
    "\n",
    "_ = session.sql('CREATE OR REPLACE TABLE '+str(clone_table_name)+\" CLONE \"+str(trips_table_name)).collect()\n",
    "_ = session.sql('CREATE TAG IF NOT EXISTS model_id_tag').collect()\n",
    "_ = session.sql(\"ALTER TABLE \"+str(clone_table_name)+\" SET TAG model_id_tag = '\"+str(model_id)+\"'\").collect()\n",
    "_ = session.sql('DROP TABLE IF EXISTS '+pred_table_name).collect()\n",
    "_ = session.sql('DROP TABLE IF EXISTS '+eval_table_name).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/code/dags/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/code/notebooks',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.8/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/code/dags/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citibike_ml.ingest import incremental_elt\n",
    "from citibike_ml.mlops_pipeline import deploy_pred_train_udf\n",
    "from citibike_ml.mlops_pipeline import materialize_holiday_weather\n",
    "from citibike_ml.mlops_pipeline import generate_feature_views\n",
    "from citibike_ml.mlops_pipeline import train_predict_feature_views\n",
    "from citibike_ml.model_eval import deploy_eval_udf\n",
    "from citibike_ml.model_eval import evaluate_station_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incremental ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file https://s3.amazonaws.com/tripdata/201402-citibike-tripdata.zip\n",
      "Gzipping file 2014-02 - Citi Bike trip data.csv\n",
      "Putting file 201402-citibike-tripdata.gz to stage load_stage\n",
      "Downloading file https://s3.amazonaws.com/tripdata/202102-citibike-tripdata.csv.zip\n",
      "Gzipping file 202102-citibike-tripdata.csv\n",
      "Putting file 202102-citibike-tripdata.csv.gz to stage load_stage\n",
      "CPU times: user 11.7 s, sys: 500 ms, total: 12.2 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "file_name_end2 = '202102-citibike-tripdata.csv.zip'\n",
    "file_name_end1 = '201402-citibike-tripdata.zip'\n",
    "\n",
    "files_to_download = [file_name_end1, file_name_end2]\n",
    "\n",
    "trips_table_name = incremental_elt(session=session, \n",
    "                                   load_stage_name=load_stage_name, \n",
    "                                   files_to_download=files_to_download, \n",
    "                                   download_base_url=download_base_url, \n",
    "                                   load_table_name=load_table_name, \n",
    "                                   trips_table_name=trips_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 ms, sys: 0 ns, total: 34.2 ms\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model_udf_name = deploy_pred_train_udf(session=session, \n",
    "                                       function_name='station_train_predict_func', \n",
    "                                       model_stage_name=model_stage_name,\n",
    "                                      path_pytorch_tabnet=\"/code/include/\",\n",
    "                                      path_citibike_ml=\"/code/dags/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "MissingDependencyError",
     "evalue": "Missing optional dependency: pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDependencyError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/code/dags/citibike_ml/mlops_pipeline.py:11\u001b[0m, in \u001b[0;36mmaterialize_holiday_weather\u001b[0;34m(session, trips_table_name, holiday_table_name, precip_table_name, path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      8\u001b[0m start_date, end_date \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mtable(trips_table_name) \\\n\u001b[1;32m      9\u001b[0m                               \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTARTTIME\u001b[39m\u001b[38;5;124m'\u001b[39m), F\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTARTTIME\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m holiday_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_holiday_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m holiday_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(holiday_table_name)\n\u001b[1;32m     14\u001b[0m precip_df \u001b[38;5;241m=\u001b[39m generate_precip_df(session\u001b[38;5;241m=\u001b[39msession, start_date\u001b[38;5;241m=\u001b[39mstart_date, end_date\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(),path\u001b[38;5;241m=\u001b[39mpath)\n",
      "File \u001b[0;32m/code/dags/citibike_ml/feature_engineering.py:10\u001b[0m, in \u001b[0;36mgenerate_holiday_df\u001b[0;34m(session, start_date, end_date)\u001b[0m\n\u001b[1;32m      8\u001b[0m holiday_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cal\u001b[38;5;241m.\u001b[39mholidays(start\u001b[38;5;241m=\u001b[39mstart_date, end\u001b[38;5;241m=\u001b[39mend_date), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(holiday_list))\n\u001b[0;32m---> 10\u001b[0m holiday_df \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mholiday_list\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     11\u001b[0m                     \u001b[38;5;241m.\u001b[39mtoDF(holiday_list\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]) \\\n\u001b[1;32m     12\u001b[0m                     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHOLIDAY\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m holiday_df\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/snowflake/snowpark/session.py:736\u001b[0m, in \u001b[0;36mSession.createDataFrame\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Row):\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreateDataFrame() function does not accept a Row object.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m)):\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreateDataFrame() function only accepts data as a list, tuple or a pandas DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m     )\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# check to see if it is a Pandas DataFrame and if so, write that to a temp\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# table and return as a DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/snowflake/connector/options.py:36\u001b[0m, in \u001b[0;36mMissingOptionalDependency.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingDependencyError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dep_name)\n",
      "\u001b[0;31mMissingDependencyError\u001b[0m: Missing optional dependency: pandas"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "holiday_table_name, precip_table_name = materialize_holiday_weather(session=session, \n",
    "                                                                   trips_table_name=trips_table_name, \n",
    "                                                                   holiday_table_name=holiday_table_name, \n",
    "                                                                   precip_table_name=precip_table_name,\n",
    "                                                                   path=\"/code/include/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "feature_view_names = generate_feature_views(session=session, \n",
    "                                            clone_table_name=clone_table_name, \n",
    "                                            feature_view_name=feature_view_name, \n",
    "                                            holiday_table_name=holiday_table_name, \n",
    "                                            precip_table_name=holiday_table_name,\n",
    "                                            target_column='COUNT', \n",
    "                                            top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "pred_table_name = train_predict_feature_views(session=session, \n",
    "                                              station_train_pred_udf_name=model_udf_name, \n",
    "                                              feature_view_names=feature_view_names, \n",
    "                                              pred_table_name=pred_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_model_udf_name = deploy_eval_udf(session=session, \n",
    "                                      function_name='eval_model_output_func', \n",
    "                                      model_stage_name=model_stage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_table_name = evaluate_station_predictions(session=session, \n",
    "                                               pred_table_name=pred_table_name, \n",
    "                                               eval_model_udf_name=eval_model_udf_name, \n",
    "                                               eval_table_name=eval_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(feature_view_names[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(pred_table_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(eval_table_name).show()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
