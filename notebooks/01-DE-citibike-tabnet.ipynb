{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "We begin where all ML use cases do: data engineering. In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API to build an **ELT pipeline**.  We will extract the data from the source system (s3), load it into snowflake and add transformations to clean the data before analysis. \n",
    "\n",
    "The data engineer has been told that there is historical data going back to 2013 and new data will be made available at the end of each month. \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The data engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the data engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models.  \n",
    "\n",
    "Input: Historical bulk data at `https://s3.amazonaws.com/tripdata/`. Incremental data to be loaded one month at a time.  \n",
    "Output: `trips` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.WARN)\n",
    "#logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize a simple json file to store our credentials. This should **never** be done in production and is for demo purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, getpass\n",
    "with open('creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    connection_parameters = {\n",
    "      'account': data['account'],\n",
    "      'user': data['username'],\n",
    "      'password': data['password'], #getpass.getpass(),\n",
    "      'role': data['role'],\n",
    "      'warehouse': data['warehouse']}\n",
    "\n",
    "session = snp.Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create objects in Snowflake\n",
    "With Snowpark, we can utilize our session object to execute DDL SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_base_url = 'https://s3.amazonaws.com/tripdata/'\n",
    "\n",
    "project_db_name = 'CITIBIKEML'\n",
    "project_schema_name = 'DEMO'\n",
    "project_db_schema = str(project_db_name)+'.'+str(project_schema_name)\n",
    "\n",
    "load_table_name = str(project_db_schema)+'.'+'RAW_'\n",
    "trips_table_name = str(project_db_schema)+'.'+'TRIPS'\n",
    "\n",
    "_ = session.sql('CREATE OR REPLACE DATABASE '+str(project_db_name)).collect()\n",
    "_ = session.sql('USE DATABASE '+str(project_db_name)).collect()\n",
    "\n",
    "_ = session.sql('CREATE SCHEMA '+str(project_db_schema)).collect()\n",
    "_ = session.sql('USE SCHEMA '+str(project_db_schema)).collect()\n",
    "\n",
    "import uuid \n",
    "stage_id = str(uuid.uuid1()).replace('-', '_')\n",
    "load_stage_name = 'load_stage_'+str(stage_id)\n",
    "\n",
    "session.sql('CREATE OR REPLACE TEMPORARY STAGE '+load_stage_name).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract:  Load Historical Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of files to download and upload to stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#For files like 201306-citibike-tripdata.zip\n",
    "date_range1 = pd.period_range(start=datetime.strptime(\"201306\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"201612\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end1 = '-citibike-tripdata.zip'\n",
    "files_to_download = [date+file_name_end1 for date in date_range1.to_list()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in January 2017 Citibike changed the format of the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For files like 201701-citibike-tripdata.csv.zip\n",
    "date_range2 = pd.period_range(start=datetime.strptime(\"201701\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"202002\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end2 = '-citibike-tripdata.csv.zip'\n",
    "files_to_download = files_to_download + [date+file_name_end2 for date in date_range2.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For development purposes we will start with loading just a couple of files.  We will create a bulk load process afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = [files_to_download[i] for i in [0,14,18,19,50]]\n",
    "files_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "files_to_load = list()\n",
    "for zip_file_name in files_to_download:\n",
    "    gz_file_name = os.path.splitext(zip_file_name)[0]+'.gz'\n",
    "    url = download_base_url+zip_file_name\n",
    "    \n",
    "    print('Downloading: '+url)\n",
    "    r = requests.get(url)\n",
    "    with open(zip_file_name, 'wb') as fh:\n",
    "        fh.write(r.content)\n",
    "        \n",
    "    with ZipFile(zip_file_name, 'r') as zipObj:\n",
    "        csv_file_names = zipObj.namelist()\n",
    "        with zipObj.open(name=csv_file_names[0], mode='r') as zf:\n",
    "            print('Gzipping: '+csv_file_names[0])\n",
    "            with gzip.open(gz_file_name, 'wb') as gzf:\n",
    "                gzf.write(zf.read())\n",
    "\n",
    "    print('Putting to stage: '+gz_file_name)\n",
    "    session.file.put(gz_file_name, '@'+load_stage_name)\n",
    "    files_to_load.append(gz_file_name)\n",
    "    os.remove(zip_file_name)\n",
    "    os.remove(gz_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"list @\"+load_stage_name+\" pattern='.*20.*[.]gz'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ELT - Load into raw table\n",
    "Load raw as all string type.  We will fix data types in the transform stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_schema1 = T.StructType([T.StructField(\"TRIPDURATION\", T.StringType()),\n",
    "                           T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                           T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                           T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                           T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                           T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                           T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                           T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                           T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                           T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                           T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                           T.StructField(\"BIKEID\", T.StringType()),\n",
    "                           T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                           T.StructField(\"BIRTH_YEAR\", T.StringType()),\n",
    "                           T.StructField(\"GENDER\", T.StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                     .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                     .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                     .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                     .option(\"NULL_IF\", \"NULL\")\\\n",
    "                     .option(\"pattern\", \"'.*20.*[.]gz'\")\\\n",
    "                     .schema(load_schema1)\\\n",
    "                     .csv(\"@\"+load_stage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "print('Loading '+str(loaddf.count())+' records to table '+load_table_name+str('schema1'))\n",
    "loaddf.copy_into_table(load_table_name+str('schema1'), \n",
    "                       files=files_to_load, \n",
    "                       format_type_options=csv_file_format_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ELT - Transform and load to raw table\n",
    "We have the raw data loaded. Now let's transform this data and clean it up. This will push the data to a final \\\"transformed\\\" table to be consumed by our Data Science team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transdf = session.table(load_table_name+'schema1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different date formats \"2014-08-10 15:21:22\", \"1/1/2015 1:30\" and \"12/1/2014 02:04:53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format_2 = \"1/1/2015 [0-9]:.*$\"      #1/1/2015 1:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_3 = \"1/1/2015 [0-9][0-9]:.*$\" #1/1/2015 10:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_4 = \"12/1/2014.*\"             #12/1/2014 02:04:53 -> M*M/D*D/YYYY \n",
    "\n",
    "#Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transdf.withColumn('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .withColumn('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "      .withColumn('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .withColumn('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "      .select(F.col('STARTTIME'), \n",
    "              F.col('STOPTIME'), \n",
    "              F.col('START_STATION_ID'), \n",
    "              F.col('START_STATION_NAME'), \n",
    "              F.col('START_STATION_LATITUDE'), \n",
    "              F.col('START_STATION_LONGITUDE'), \n",
    "              F.col('END_STATION_ID'), \n",
    "              F.col('END_STATION_NAME'), F.col('END_STATION_LATITUDE'), \n",
    "              F.col('END_STATION_LONGITUDE'), \n",
    "              F.col('USERTYPE'))\\\n",
    "      .write.mode('overwrite').saveAsTable(trips_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = session.table(trips_table_name)\n",
    "testdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export code in functional modules for MLOps and orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile citibike_ml/elt.py\n",
    "\n",
    "def extract_trips_to_stage(session, files_to_download: list, download_base_url: str, load_stage_name:str):\n",
    "    import os \n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    import gzip\n",
    "    \n",
    "    files_to_load = list()\n",
    "    \n",
    "    for zip_file_name in files_to_download:\n",
    "        gz_file_name = os.path.splitext(zip_file_name)[0]+'.gz'\n",
    "        url = download_base_url+zip_file_name\n",
    "\n",
    "        print('Downloading file '+url)\n",
    "        r = requests.get(url)\n",
    "        with open(zip_file_name, 'wb') as fh:\n",
    "            fh.write(r.content)\n",
    "\n",
    "        with ZipFile(zip_file_name, 'r') as zipObj:\n",
    "            csv_file_names = zipObj.namelist()\n",
    "            with zipObj.open(name=csv_file_names[0], mode='r') as zf:\n",
    "                print('Gzipping file '+csv_file_names[0])\n",
    "                with gzip.open(gz_file_name, 'wb') as gzf:\n",
    "                    gzf.write(zf.read())\n",
    "\n",
    "        print('Putting file '+gz_file_name+' to stage '+load_stage_name)\n",
    "        session.file.put(gz_file_name, '@'+load_stage_name)\n",
    "        \n",
    "        files_to_load.append(gz_file_name)\n",
    "        os.remove(zip_file_name)\n",
    "        os.remove(gz_file_name)\n",
    "    \n",
    "    return load_stage_name, files_to_load\n",
    "        \n",
    "def load_trips_to_raw(session, files_to_load:list, load_stage_name:str, load_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "    from datetime import datetime\n",
    "\n",
    "    stage_table_names = list()\n",
    "    schema1_files = list()\n",
    "    schema2_files = list()\n",
    "    schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "    \n",
    "    for file_name in files_to_load:\n",
    "        file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "        if file_start_date < schema2_start_date:\n",
    "            schema1_files.append(file_name)\n",
    "        else:\n",
    "            schema2_files.append(file_name)\n",
    "\n",
    "    if len(schema1_files) > 0:\n",
    "        load_schema1 = T.StructType([T.StructField(\"tripduration\", T.StringType()),\n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"BIKEID\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                             T.StructField(\"birth_year\", T.StringType()),\n",
    "                             T.StructField(\"gender\", T.StringType())])\n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "        \n",
    "        stage_table_name = load_table_name + str('schema1')\n",
    "        \n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema1)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema1_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "\n",
    "\n",
    "    if len(schema2_files) > 0:\n",
    "        load_schema2 = T.StructType([T.StructField(\"ride_id\", T.StringType()), \n",
    "                             T.StructField(\"rideable_type\", T.StringType()), \n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "        stage_table_name = load_table_name + str('schema2')\n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema2)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema2_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "        \n",
    "    return list(set(stage_table_names))\n",
    "    \n",
    "def transform_trips(session, stage_table_names:list, trips_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "        \n",
    "    #Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "    date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "    date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\"\n",
    "    \n",
    "    for stage_table_name in stage_table_names:\n",
    "        \n",
    "        transdf = session.table(stage_table_name)\n",
    "        transdf.withColumn('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                                F.lit(date_format_match), \n",
    "                                                F.lit(date_format_repl)))\\\n",
    "               .withColumn('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "               .withColumn('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                                F.lit(date_format_match), \n",
    "                                                F.lit(date_format_repl)))\\\n",
    "               .withColumn('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "               .select(F.col('STARTTIME'), \n",
    "                       F.col('STOPTIME'), \n",
    "                       F.col('START_STATION_ID'), \n",
    "                       F.col('START_STATION_NAME'), \n",
    "                       F.col('START_STATION_LATITUDE'), \n",
    "                       F.col('START_STATION_LONGITUDE'), \n",
    "                       F.col('END_STATION_ID'), \n",
    "                       F.col('END_STATION_NAME'), F.col('END_STATION_LATITUDE'), \n",
    "                       F.col('END_STATION_LONGITUDE'), \n",
    "                       F.col('USERTYPE'))\\\n",
    "               .write.saveAsTable(trips_table_name)\n",
    "\n",
    "    return trips_table_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: There are duplicate station_id and station_name.  Needs cleaning. We use station_id for analysis so need to cleanup.\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
