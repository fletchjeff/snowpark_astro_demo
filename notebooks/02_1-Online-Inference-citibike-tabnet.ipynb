{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Citibike Trips Using Snowflake [Snowpark](https://docs.snowflake.com/en/LIMITEDACCESS/snowpark-python.html) & Tabnet\n",
    "Here we will use the model that we created previously as a UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-03-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "\n",
    "Make sure to install the following dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q '../snowflake_snowpark_python-0.2.0-py3-none-any.whl[pandas]'\n",
    "!pip install -q pandas toml matplotlib seaborn pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "#logging.getLogger().setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from os import path\n",
    "homedir = path.expanduser(\"~\")\n",
    "\n",
    "snf_env = toml.load(path.join(homedir, '.snowflake_config.toml')).get('citibike')\n",
    "\n",
    "session = snp.session.Session.builder.configs(snf_env)\n",
    "session = session.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets update the feature functions with holiday and precip features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(snowdf, station_id):\n",
    "    agg_period = 'DAY'\n",
    "\n",
    "    start_date, end_date = snowdf.select(F.min('STARTTIME'), F.max('STARTTIME')).collect()[0][0:2]\n",
    "\n",
    "    snowdf = snowdf.filter(F.col('START_STATION_ID') == station_id) \\\n",
    "                         .withColumn('DATE', \n",
    "                                     F.call_builtin('DATE_TRUNC', (agg_period, F.col('STARTTIME')))) \\\n",
    "                         .groupBy('DATE') \\\n",
    "                         .count() \n",
    "                           \n",
    "    holiday_df = session.table('HOLIDAYS')\n",
    "    precip_df = session.table('WEATHER')\n",
    "\n",
    "\n",
    "  \n",
    "    #Impute missing values for lag columns using mean of the previous period.\n",
    "    mean_1 = round(snowdf.sort('DATE').limit(1).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_7 = round(snowdf.sort('DATE').limit(7).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_30 = round(snowdf.sort('DATE').limit(30).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_90 = round(snowdf.sort('DATE').limit(90).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_365 = round(snowdf.sort('DATE').limit(365).select(F.mean('COUNT')).collect()[0][0])\n",
    "    \n",
    "    date_win = snp.Window.orderBy('DATE')\n",
    "\n",
    "    snowdf = snowdf.withColumn('LAG_1', F.lag('COUNT', offset=1, default_value=mean_1) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .withColumn('LAG_7', F.lag('COUNT', offset=7, default_value=mean_7) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .withColumn('LAG_30', F.lag('COUNT', offset=30, default_value=mean_30) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .withColumn('LAG_90', F.lag('COUNT', offset=90, default_value=mean_90) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .withColumn('LAG_365', F.lag('COUNT', offset=365, default_value=mean_365) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0}) \\\n",
    "                   .join(precip_df, 'DATE', 'inner') \\\n",
    "                   #.withColumn('DAYOFWEEK', F.call_builtin('DAYOFWEEK', F.col('DATE'))) \\\n",
    "                   #.withColumn('MONTH', F.call_builtin('MONTH', F.col('DATE'))) \\\n",
    "                   #.na.drop() \\\n",
    "                   #.sort('DATE', ascending=True) \n",
    "\n",
    "    return snowdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "We create functions to generate past and future dataframes for inference and evaluation.  These will be useful later in our inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_past_df(table, station_id, today, prediction_period):\n",
    "    \n",
    "#     newdf = session.table(table).filter((F.to_date('STARTTIME') <= F.to_date(F.lit(today))) \n",
    "#                                         &              \n",
    "#                                         (F.to_date('STARTTIME') >= F.dateadd('DAY', \n",
    "#                                                                              F.lit(-365+prediction_period), \n",
    "#                                                                              F.to_date(F.lit(today)))))\n",
    "    \n",
    "#     newdf = generate_features(newdf, station_id)\n",
    "#     newdf = newdf.filter(F.to_date('DATE') >= F.dateadd('DAY',F.lit(prediction_period),F.to_date(F.lit(today))))\n",
    "  \n",
    "#     return newdf\n",
    "\n",
    "\n",
    "def generate_future_df(table, station_id, today, prediction_period):\n",
    "    \n",
    "    newdf = session.table(table).filter((F.to_date('STARTTIME') < F.dateadd('DAY', \n",
    "                                                                            F.lit(prediction_period), \n",
    "                                                                            F.to_date(F.lit(today))))\n",
    "                                        &\n",
    "                                        (F.to_date('STARTTIME') >= F.dateadd('DAY', \n",
    "                                                                             F.lit(-365), \n",
    "                                                                             F.to_date(F.lit(today)))))\n",
    "\n",
    "    newdf = generate_features(newdf, station_id)\n",
    "    newdf = newdf.filter(F.to_date('DATE') >= F.to_date(F.lit(today)))\n",
    "\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our model performed for the previous 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "|\"DATE\"               |\"COUNT\"  |\"LAG_1\"  |\"LAG_7\"  |\"LAG_30\"  |\"LAG_90\"  |\"LAG_365\"  |\"HOLIDAY\"  |\"PRECIP\"  |\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "|2021-03-10 00:00:00  |823      |693      |631      |328       |691       |615        |0          |0.0       |\n",
      "|2021-03-11 00:00:00  |761      |823      |651      |376       |653       |587        |0          |0.38      |\n",
      "|2021-03-12 00:00:00  |760      |761      |608      |423       |624       |606        |0          |0.0       |\n",
      "|2021-03-13 00:00:00  |789      |760      |668      |395       |555       |655        |0          |0.0       |\n",
      "|2021-03-14 00:00:00  |761      |789      |603      |389       |471       |635        |0          |0.0       |\n",
      "|2021-03-15 00:00:00  |752      |761      |648      |380       |533       |581        |0          |0.0       |\n",
      "|2021-03-16 00:00:00  |836      |752      |693      |392       |620       |460        |0          |0.97      |\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets predict the next 7 days\n",
    "prediction_period=7\n",
    "\n",
    "future_df = generate_future_df(table='trips_stations_vw', \n",
    "             station_id='2006', \n",
    "             today=today, \n",
    "             prediction_period=prediction_period)\n",
    "\n",
    "future_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"LAG_1\"  |\"LAG_7\"  |\"LAG_30\"  |\"LAG_90\"  |\"LAG_365\"  |\"HOLIDAY\"  |\"PRECIP\"  |\n",
      "------------------------------------------------------------------------------\n",
      "|693      |631      |328       |691       |615        |0          |0.0       |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = ['COUNT']\n",
    "feature_columns = [feature.replace('\\\"', '') for feature in future_df.columns]\n",
    "feature_columns.remove(target[0])\n",
    "feature_columns.remove('DATE')\n",
    "\n",
    "future_df.select(feature_columns).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "|\"STATION_2006_MODEL_UDF(377, 427, 550, 1016, 10...  |\n",
      "------------------------------------------------------\n",
      "|452                                                 |\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.range(1).select(F.call_udf('station_2006_model_udf',\n",
    "                                   F.lit(377), \n",
    "                                   F.lit(427), \n",
    "                                   F.lit(550), \n",
    "                                   F.lit(1016), \n",
    "                                   F.lit(1091), \n",
    "                                   F.lit(0), \n",
    "                                   F.lit(23.87))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "|\"DATE\"               |\"COUNT\"  |\"LAG_1\"  |\"LAG_7\"  |\"LAG_30\"  |\"LAG_90\"  |\"LAG_365\"  |\"HOLIDAY\"  |\"PRECIP\"  |\"PRED\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "|2021-03-10 00:00:00  |823      |693      |631      |328       |691       |615        |0          |0.0       |715     |\n",
      "|2021-03-11 00:00:00  |761      |823      |651      |376       |653       |587        |0          |0.38      |779     |\n",
      "|2021-03-12 00:00:00  |760      |761      |608      |423       |624       |606        |0          |0.0       |731     |\n",
      "|2021-03-13 00:00:00  |789      |760      |668      |395       |555       |655        |0          |0.0       |777     |\n",
      "|2021-03-14 00:00:00  |761      |789      |603      |389       |471       |635        |0          |0.0       |741     |\n",
      "|2021-03-15 00:00:00  |752      |761      |648      |380       |533       |581        |0          |0.0       |753     |\n",
      "|2021-03-16 00:00:00  |836      |752      |693      |392       |620       |460        |0          |0.97      |797     |\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_df.withColumn('PRED', F.call_udf('station_2006_model_udf', [F.col(c) for c in feature_columns])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adventures\n",
    "#### impute missing instead of drop\n",
    "#### add additional lag features\n",
    "#### check feature importance\n",
    "#### build with most important features\n",
    "#### train in UDF\n",
    "#### ci/cd pipeline\n",
    "#### Feature views and ZCC\n",
    "#### build/train/deploy loop for top N stations.\n",
    "#### parallelization\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
